# -*- coding: utf-8 -*-
"""run_frontend_in_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SB1u2uTo4J8_K_NA0EishhszhKa78sq4
"""

# download nltk stopwords
import nltk
nltk.download('stopwords')

!apt-get install git

!pip git clone https://github.com/facebookresearch/fastText.git

# Install a particular version of `google-cloud-storage` because (oddly enough) 
# the  version on Colab and GCP is old. A dependency error below is okay.
!pip install -q google-cloud-storage==1.43.0

!python download_model.py en

# authenticate below for Google Storage access as needed
from google.colab import auth
auth.authenticate_user()

# install ngrok to emulate public IP / address
!wget -N https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip
!unzip -u ngrok-stable-linux-amd64.zip

# TODO: sign up for an ngrok account
# then put your ngrok token below, uncomment, and execute
!./ngrok authtoken 22KeqcMQYpd2iRnsaZZOzlyaNw0_5NAk69pXRUMvfrf1oCNpU

# install a ngrok python package and a version of flask that works with it in 
# colab
!pip -q install flask-ngrok
!pip -q install flask==0.12.2
!pip -q install flask_restful
!pip install fasttext

"""# Run the app"""

# you need to upload your implementation of search_app.py
!pip install fasttext==0.9.2
import search_frontend as se

# uncomment the code below and execute to reload the module when you make 
# changes to search_frontend.py (after you upload again).
import importlib
importlib.reload(se)

from flask_ngrok import run_with_ngrok
run_with_ngrok(se.app) 
se.app.run()

"""# Testing your app

Once your app is running you can query it. You can simply do that by navigating to the URL that ngrok gave you above or through code in a different python session. For example, once the frontend app is running, you can navigate to:
http://YOUR_SERVER_DOMAIN/search?query=hello+world where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io, which is printed above in Colab or that is your external IP on GCP.

The code below shows how to issue a query from python. This is also how our testing code will issue queries to your search engine, so make sure to test your search engine this way after you deploy it to GCP and before submission. Command line instructions for deploying your search engine to GCP are available at `run_frontend_in_gcp.sh`. Note that we will not only issue training queries to your search engine, but also test queries, i.e. queries that you've never seen before.
"""

import json

with open('queries_train.json', 'rt') as f:
  queries = json.load(f)

def average_precision(true_list, predicted_list, k=40):
    true_set = frozenset(true_list)
    predicted_list = predicted_list[:k]
    precisions = []
    for i,doc_id in enumerate(predicted_list):        
        if doc_id in true_set:
            prec = (len(precisions)+1) / (i+1)            
            precisions.append(prec)
    if len(precisions) == 0:
        return 0.0
    return round(sum(precisions)/len(precisions),3)

import numpy as np
import random
import requests
from time import time
from scipy.optimize import minimize

url = 'http://34.27.134.71:8080'

# Define a function to calculate the total AP for the current parameter values
def calculate_ap(params):
    ap_total = 0
    for q, true_wids in queries.items():
        try:
            res = requests.get(url + '/search', {'query': q,'b':params[0],'k1':params[1],'k3':params[2],'k4':params[3],'k5':params[4]}, timeout=35)
            if res.status_code == 200:
                pred_wids, _ = zip(*res.json())
                ap = average_precision(true_wids, pred_wids)
                ap_total += ap
        except:
            pass
    print(ap_total)
    return -ap_total # negate because we want to maximize AP

initial_guess_1 = [90, 1350, 8, 4.5, 30]
initial_guess_2 = [80, 1250, 7, 4.0, 25]
initial_guess_3 = [100, 1450, 9, 5.0, 35]

res_1 = minimize(calculate_ap, initial_guess_1, method='BFGS')
res_2 = minimize(calculate_ap, initial_guess_2, method='BFGS')
res_3 = minimize(calculate_ap, initial_guess_3, method='BFGS')

# Compare the results
print(f'Initial guess 1: {res_1.x} - {res_1.fun}')
print(f'Initial guess 2: {res_2.x} - {res_2.fun}')
print(f'Initial guess 3: {res_3.x} - {res_3.fun}')

!pip install optuna

import numpy as np
import random
import requests
from time import time
from scipy.optimize import differential_evolution
from multiprocessing import Pool
import optuna

url = 'http://34.27.134.71:8080'


def objective(trial):
    b = trial.suggest_uniform('b', 50, 150)
    k1 = trial.suggest_uniform('k1', 1000, 1500)
    k3 = trial.suggest_uniform('k3', 5, 10)
    k4 = trial.suggest_uniform('k4', 3, 5)
    k5 = trial.suggest_uniform('k5', 20, 40)
    pv=trial.suggest_uniform('pv', 1,2)
    pr=trial.suggest_uniform('pr', 0.1,0.7)

    params = [b, k1, k3, k4, k5,pv,pr]
    ap_total = 0
    for q, true_wids in queries.items():
        try:
            res = requests.get(url + '/search', {'query': q,'b':params[0],'k1':params[1],'k3':params[2],'k4':params[3],'k5':params[4],'pr':params[5],'pv':params[6]}, timeout=35)
            if res.status_code == 200:
                pred_wids, _ = zip(*res.json())
                ap = average_precision(true_wids, pred_wids)
                ap_total += ap
        except:
            pass
    print(ap_total)
    print(b,k1,k3,k4,k5)
    return -ap_total

study = optuna.create_study()
study.optimize(objective, n_trials=100)

import optuna
b_range = [50,150]
k1_range = [500,2000]
k3_range = [5,30]
k5_range = [5,60]
k4_range = [1,16]
pr_range = [1.0,2.0]
pv_range = [0.1,0.7]
def objective(trial):
    b = trial.suggest_uniform('b', 50, 150)
    k1 = trial.suggest_uniform('k1', 1000, 1500)
    k3 = trial.suggest_uniform('k3', 5, 10)
    k4 = trial.suggest_uniform('k4', 3, 5)
    k5 = trial.suggest_uniform('k5', 20, 40)
    pv=trial.suggest_uniform('pv', 1,2)
    pr=trial.suggest_uniform('pr', 0.1,0.7)

    params = [b, k1, k3, k4, k5,pv,pr]
    ap_total = 0
    for q, true_wids in queries.items():
        try:
            res = requests.get(url + '/search', {'query': q,'b':params[0],'k1':params[1],'k3':params[2],'k4':params[3],'k5':params[4],'pr':params[5],'pv':params[6]}, timeout=35)
            if res.status_code == 200:
                pred_wids, _ = zip(*res.json())
                ap = average_precision(true_wids, pred_wids)
                ap_total += ap
        except:
            pass
    return -ap_total

study = optuna.create_study
study.optimize(objective, n_trials=100)

print("Best parameters:", params)
print(total_ap - prev_ap)

import random
import requests
from time import time
url = 'http://104.197.128.88:8080'
b_range = [50,150]
k1_range = [500,2000]
k3_range = [5,30]
k5_range = [5,60]
k4_range = [1,16]
best_ap = 0
best_params = {}
for i in range(100):
  print(best_ap)
  print(i)
  b = random.choice(b_range)
  k1 = random.choice(k1_range)
  k4= random.choice(k4_range)
  k3= random.choice(k3_range)
  k5= random.choice(k5_range)
  ap_total =0
  qs_res = []
  for q, true_wids in queries.items():
    duration, ap = None, None
    t_start = time()
    try:
      res = requests.get(url + '/search', {'query': q,'b':b,'k1':k1,'k3':k3,'k4':k4,'k5':k5}, timeout=35)
      duration = time() - t_start
      if res.status_code == 200:
        pred_wids, _ = zip(*res.json())
        ap = average_precision(true_wids, pred_wids)
        ap_total += ap
    except:
      pass
    qs_res.append((q, duration, ap))
  if best_ap < ap_total:
    best_ap = ap_total
    best_params[best_ap] = (k1,b,k3,k4,k5)

print( qs_res)
print(best_ap/len(queries))
print(best_params)

