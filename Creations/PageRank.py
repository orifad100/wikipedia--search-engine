#!/usr/bin/env python
# coding: utf-8

# ***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!

# In[1]:


# if the following command generates an error, you probably didn't enable
# the cluster security option "Allow API access to all Google Cloud services"
# under Manage Security â†’ Project Access when setting up the cluster
get_ipython().system('gcloud dataproc clusters list --region us-central1')

# # Imports & Setup

# In[2]:


get_ipython().system('pip install -q google-cloud-storage==1.43.0')
get_ipython().system('pip install -q graphframes')

# In[3]:


import pyspark
import sys
from collections import Counter, OrderedDict, defaultdict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import nltk
from nltk.stem.porter import *
from nltk.corpus import stopwords
from time import time
from pathlib import Path
import pickle
import pandas as pd
from google.cloud import storage

import hashlib


def _hash(s):
    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()


nltk.download('stopwords')

# In[4]:


# if nothing prints here you forgot to include the initialization script when starting the cluster
get_ipython().system('ls -l /usr/lib/spark/jars/graph*')

# In[5]:


from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf, SparkFiles
from pyspark.sql import SQLContext
from graphframes import *

# In[6]:


spark

# In[7]:


# Put your bucket name below and make sure you can access it without an error
import fnmatch

# Put your bucket name below and make sure you can access it without an error
bucket_name = 'ori318194230'
full_path = f"gs://{bucket_name}/"
paths = []

client = storage.Client()
blobs = client.list_blobs(bucket_name)
for b in blobs:
    if b.name != 'graphframes.sh' and not fnmatch.fnmatch(b.name, 'postings_gcp_text*') and not fnmatch.fnmatch(b.name, 'postings_gcp_anchor*') and not fnmatch.fnmatch(
            b.name, 'pr*'):
        paths.append(full_path + b.name)
# # PageRank

# *YOUR TASK (10 POINTS):* Compute PageRank for the entire English Wikipedia. Use your implementation for `generate_graph` function from Colab below.

# In[20]:


# Put your `generate_graph` function here
def generate_graph(pages):
  ''' Compute the directed graph generated by wiki links.
  Parameters:
  -----------
    pages: RDD
      An RDD where each row consists of one wikipedia articles with 'id' and
      'anchor_text'.
  Returns:
  --------
    edges: RDD
      An RDD where each row represents an edge in the directed graph created by
      the wikipedia links. The first entry should the source page id and the
      second entry is the destination page id. No duplicates should be present.
    vertices: RDD
      An RDD where each row represents a vetrix (node) in the directed graph
      created by the wikipedia links. No duplicates should be present.
  '''
  # YOUR CODE HERE
  v1=pages.map(lambda x:Row(x[0])).union(pages.flatMap(lambda x:[Row(i[0]) for i in x[1]]))
  e1=pages.flatMap(lambda x:[(x[0],i[0]) for i in x[1]])
  vertices=v1.distinct()
  edges=e1.distinct()

  return edges, vertices


# In[ ]:


t_start = time()
pages_links = spark.read.parquet(*paths).select("id", "anchor_text").rdd
# construct the graph
edges, vertices = generate_graph(pages_links)
# compute PageRank
edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')
verticesDF = vertices.toDF(['id']).repartition(124, 'id')
g = GraphFrame(verticesDF, edgesDF)
pr_results = g.pageRank(resetProbability=0.15, maxIter=6)
pr = pr_results.vertices.select("id", "pagerank")
pr = pr.sort(col('pagerank').desc())
pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression="gzip")
pr_time = time() - t_start
pr.show()


